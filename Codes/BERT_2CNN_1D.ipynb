{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-2CNN-1D.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTzmn-Dw3zwy",
        "outputId": "b0488ccd-d9bd-44b8-ab9b-adbf0342ec16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT-2CNN-1D Model"
      ],
      "metadata": {
        "id": "KkTl5_WU-CXB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqztgJwamZao",
        "outputId": "2a92c6e7-fb62-46ff-8613-682d6a40bd5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.8.0rc4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.0.53)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.1.96)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2022.6.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.0) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.7.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==3.0.0\n",
        "!pip install emoji\n",
        "\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "class BERT_2CNN_1D(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BERT_2CNN_1D, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "    self.conv = nn.Conv1d(in_channels=768, out_channels=64, kernel_size=3, padding=\"same\")\n",
        "    self.conv2 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=\"same\")\n",
        "    self.pool = nn.MaxPool1d(kernel_size=3, stride=1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.fc = nn.Linear(1024, 2) # 32*(36-2-2) = 1024\n",
        "    self.flat = nn.Flatten()\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, sent_id, mask): \n",
        "    last_hidden_state, _ = self.bert(sent_id, attention_mask=mask)\n",
        "    x = last_hidden_state\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    x = torch.transpose(x, 1, 2)\n",
        "    x = self.pool(self.dropout(self.relu(self.conv(self.dropout(x)))))\n",
        "    x = self.pool(self.dropout(self.relu(self.conv2(self.dropout(x)))))\n",
        "    x = self.fc(self.dropout(self.flat(self.dropout(x))))\n",
        "    return self.softmax(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the dataset and Tokenization\n",
        "The pre_process_dataset(values) and data_process(data,labels) methods have been implemented by [@ZeroxTM](https://github.com/ZeroxTM) in [this repository](https://github.com/ZeroxTM/BERT-CNN-Fine-Tuning-For-Hate-Speech-Detection-in-Online-Social-Media) the methods are in Pre_Process.py."
      ],
      "metadata": {
        "id": "ZUACq5xf-QTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import emoji\n",
        "\n",
        "DATASET_PATH = '/content/drive/MyDrive/NLP-project/dataset/train.csv'\n",
        "\n",
        "def balance_dataset(dataset):\n",
        "    min_len = (dataset['label']==1).sum()\n",
        "    sampled_from_0s = dataset[dataset['label']==0].sample(n = 2*min_len, random_state=42)\n",
        "    return pd.concat([dataset[dataset['label']==1], sampled_from_0s])\n",
        "\n",
        "def read_dataset(path):\n",
        "    dataset = pd.read_csv(path)\n",
        "    dataset = dataset[~pd.isna(dataset['comment_text'])]\n",
        "    dataset['label'] = dataset.values[:,2:].sum(axis=1) > 0\n",
        "    dataset['label'] = dataset['label'].astype(int)\n",
        "    balanced_dataset = balance_dataset(dataset)\n",
        "    return balanced_dataset['comment_text'].tolist(), balanced_dataset['label']\n",
        "\n",
        "def data_process(data, labels):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    for sentence in data:\n",
        "        bert_inp = bert_tokenizer.__call__(sentence, max_length=36,\n",
        "                                           padding='max_length', pad_to_max_length=True,\n",
        "                                           truncation=True, return_token_type_ids=False)\n",
        "\n",
        "        input_ids.append(bert_inp['input_ids'])\n",
        "        attention_masks.append(bert_inp['attention_mask'])\n",
        "    input_ids = np.asarray(input_ids)\n",
        "    attention_masks = np.array(attention_masks)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return input_ids, attention_masks, labels\n",
        "\n",
        "def pre_process_dataset(values):\n",
        "    new_values = list()\n",
        "    # Emoticons\n",
        "    emoticons = [':-)', ':)', '(:', '(-:', ':))', '((:', ':-D', ':D', 'X-D', 'XD', 'xD', 'xD', '<3', '</3', ':\\*',\n",
        "                 ';-)',\n",
        "                 ';)', ';-D', ';D', '(;', '(-;', ':-(', ':(', '(:', '(-:', ':,(', ':\\'(', ':\"(', ':((', ':D', '=D',\n",
        "                 '=)',\n",
        "                 '(=', '=(', ')=', '=-O', 'O-=', ':o', 'o:', 'O:', 'O:', ':-o', 'o-:', ':P', ':p', ':S', ':s', ':@',\n",
        "                 ':>',\n",
        "                 ':<', '^_^', '^.^', '>.>', 'T_T', 'T-T', '-.-', '*.*', '~.~', ':*', ':-*', 'xP', 'XP', 'XP', 'Xp',\n",
        "                 ':-|',\n",
        "                 ':->', ':-<', '$_$', '8-)', ':-P', ':-p', '=P', '=p', ':*)', '*-*', 'B-)', 'O.o', 'X-(', ')-X']\n",
        "\n",
        "    for value in values:\n",
        "        # Remove dots\n",
        "        text = value.replace(\".\", \"\").lower()\n",
        "        text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
        "        users = re.findall(\"[@]\\w+\", text)\n",
        "        for user in users:\n",
        "            text = text.replace(user, \"<user>\")\n",
        "        urls = re.findall(r'(https?://[^\\s]+)', text)\n",
        "        if len(urls) != 0:\n",
        "            for url in urls:\n",
        "                text = text.replace(url, \"<url >\")\n",
        "        for emo in text:\n",
        "            if emo in emoji.UNICODE_EMOJI:\n",
        "                text = text.replace(emo, \"<emoticon >\")\n",
        "        for emo in emoticons:\n",
        "            text = text.replace(emo, \"<emoticon >\")\n",
        "        numbers = re.findall('[0-9]+', text)\n",
        "        for number in numbers:\n",
        "            text = text.replace(number, \"<number >\")\n",
        "        text = text.replace('#', \"<hashtag >\")\n",
        "        text = re.sub(r\"([?.!,¿])\", r\" \", text)\n",
        "        text = \"\".join(l for l in text if l not in string.punctuation)\n",
        "        text = re.sub(r'[\" \"]+', \" \", text)\n",
        "        new_values.append(text)\n",
        "    return new_values\n",
        "\n",
        "def load_and_process(path):\n",
        "    data, labels = read_dataset(path)\n",
        "    num_of_labels = len(labels.unique())\n",
        "    input_ids, attention_masks, labels = data_process(pre_process_dataset(data), labels)\n",
        "\n",
        "    return input_ids, attention_masks, labels\n"
      ],
      "metadata": {
        "id": "QgW8qPOZ4KUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Phase"
      ],
      "metadata": {
        "id": "ZVaYwn8Y-abo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total = len(train_dataloader)\n",
        "\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        step = i+1\n",
        "        percent = \"{0:.3f}\".format(100 * (step / float(total)))\n",
        "        lossp = \"{0:.3f}\".format(total_loss/(step*batch_size))\n",
        "        filledLength = int(100 * step // total)\n",
        "        bar = '█' * filledLength + '>'  *(filledLength < 100) + '.' * (99 - filledLength)\n",
        "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "        del batch\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        model.zero_grad()\n",
        "        preds = model(sent_id.to(device).long(), mask)\n",
        "        loss = cross_entropy(preds, labels)\n",
        "        total_loss += float(loss.item())\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = total_loss / (len(train_dataloader)*batch_size)\n",
        "    \n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "    print(\"\\n\\nEvaluating...\")\n",
        "    model.eval()\n",
        "    total_loss= 0\n",
        "    total = len(val_dataloader)\n",
        "    for i, batch in enumerate(val_dataloader):\n",
        "        step = i+1\n",
        "        percent = \"{0:.3f}\".format(100 * (step / float(total)))\n",
        "        lossp = \"{0:.3f}\".format(total_loss/(step*batch_size))\n",
        "        filledLength = int(100 * step // total)\n",
        "        bar = '█' * filledLength + '>' * (filledLength < 100) + '.' * (99 - filledLength)\n",
        "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
        "        # push the batch to gpu\n",
        "        batch = [t.to(device) for t in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "        del batch\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        with torch.no_grad():\n",
        "            preds = model(sent_id, mask)\n",
        "            loss = cross_entropy(preds, labels)\n",
        "            total_loss += float(loss.item())\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    val_avg_loss = total_loss / (len(val_dataloader)*batch_size)\n",
        "\n",
        "    return val_avg_loss\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load the dataset\n",
        "input_ids, attention_masks, labels = load_and_process(DATASET_PATH)\n",
        "df = pd.DataFrame(list(zip(input_ids, attention_masks)), columns=['input_ids', 'attention_masks'])\n",
        "\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df, labels,\n",
        "                             random_state=42, test_size=0.2, stratify=labels)\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
        "                         random_state=42, test_size=0.5, stratify=temp_labels)\n",
        "\n",
        "del temp_text\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "train_count = len(train_labels)\n",
        "test_count = len(test_labels)\n",
        "val_count = len(val_labels)\n",
        "\n",
        "\n",
        "# for train set\n",
        "train_seq = torch.tensor(train_text['input_ids'].tolist())\n",
        "train_mask = torch.tensor(train_text['attention_masks'].tolist())\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(val_text['input_ids'].tolist())\n",
        "val_mask = torch.tensor(val_text['attention_masks'].tolist())\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(test_text['input_ids'].tolist())\n",
        "test_mask = torch.tensor(test_text['attention_masks'].tolist())\n",
        "test_y = torch.tensor(test_labels.tolist())\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "model = BERT_2CNN_1D()\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Adam optimizer\n",
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "cross_entropy = nn.NLLLoss()\n",
        "\n",
        "epochs = 3\n",
        "current = 1\n",
        "# for each epoch\n",
        "while current <= epochs:\n",
        "\n",
        "    print(f'\\nEpoch {current} / {epochs}:')\n",
        "\n",
        "    # train model\n",
        "    train_loss = train()\n",
        "\n",
        "    # evaluate model\n",
        "    valid_loss = evaluate()\n",
        "    \n",
        "    print(f'\\n\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')\n",
        "\n",
        "    current = current + 1\n"
      ],
      "metadata": {
        "id": "DegSd7e6-mJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de614f3d-71f2-4e17-d926-2781c16b1184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 / 3:\n",
            "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.007\n",
            "\n",
            "Evaluating...\n",
            "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.007\n",
            "\n",
            "Training Loss: 0.007\n",
            "Validation Loss: 0.007\n",
            "\n",
            "Epoch 2 / 3:\n",
            "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.005\n",
            "\n",
            "Evaluating...\n",
            "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.006\n",
            "\n",
            "Training Loss: 0.005\n",
            "Validation Loss: 0.006\n",
            "\n",
            "Epoch 3 / 3:\n",
            "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.003\n",
            "\n",
            "Evaluating...\n",
            "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.007\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "Iv80vf9VcWS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = model(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "print(\"Performance:\")\n",
        "# model's performance\n",
        "preds = np.argmax(preds, axis=1)\n",
        "print('Classification Report')\n",
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZtcZbXIcVn9",
        "outputId": "ee1ae448-9918-4306-b82a-4d67c65dde6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance:\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.95      0.94      3245\n",
            "           1       0.89      0.87      0.88      1623\n",
            "\n",
            "    accuracy                           0.92      4868\n",
            "   macro avg       0.91      0.91      0.91      4868\n",
            "weighted avg       0.92      0.92      0.92      4868\n",
            "\n"
          ]
        }
      ]
    }
  ]
}