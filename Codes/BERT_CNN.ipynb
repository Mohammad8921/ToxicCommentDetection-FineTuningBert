{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkxEV3kzj0qM",
        "outputId": "315aa56c-5632-4a98-c324-37f2c5c06a4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evJHanGdjziV"
      },
      "source": [
        "## BERT-CNN Model\n",
        "This model has been proposed in <https://link.springer.com/chapter/10.1007/978-3-030-36687-2_77 /> and impelemeted by [@ZeroxTM](https://github.com/ZeroxTM) in [this repository](https://github.com/ZeroxTM/BERT-CNN-Fine-Tuning-For-Hate-Speech-Detection-in-Online-Social-Media) \n",
        "\n",
        "By Alaa Grable\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A29KnyLfjwu6",
        "outputId": "bad381ef-f0a0-438e-888d-6d4ef34c045f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.1.96)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2022.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (1.21.6)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.8.0rc4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (4.64.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.0.53)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.0) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.7.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==3.0.0\n",
        "!pip install emoji\n",
        "\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "class BERT_CNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BERT_CNN, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.conv = nn.Conv2d(in_channels=13, out_channels=13, kernel_size=(3, 768), padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=3, stride=1)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(442, 3)\n",
        "        self.flat = nn.Flatten()\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, sent_id, mask):\n",
        "        _, _, all_layers = self.bert(sent_id, attention_mask=mask, output_hidden_states=True)\n",
        "        # all_layers  = [13, 32, 64, 768]\n",
        "        x = torch.transpose(torch.cat(tuple([t.unsqueeze(0) for t in all_layers]), 0), 0, 1)\n",
        "        del all_layers\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        x = self.pool(self.dropout(self.relu(self.conv(self.dropout(x)))))\n",
        "        x = self.fc(self.dropout(self.flat(self.dropout(x))))\n",
        "        return self.softmax(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_P20QUvkfeg"
      },
      "source": [
        "## Importing the dataset and tokenization \n",
        "The pre_process_dataset(values) and data_process(data,labels) methods have been implemented by [@ZeroxTM](https://github.com/ZeroxTM) in [this repository](https://github.com/ZeroxTM/BERT-CNN-Fine-Tuning-For-Hate-Speech-Detection-in-Online-Social-Media) the methods are in Pre_Process.py."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQsguY0Lked0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import emoji\n",
        "\n",
        "DATASET_PATH = '/content/drive/MyDrive/NLP-project/dataset/train.csv'\n",
        "\n",
        "def balance_dataset(dataset):\n",
        "    min_len = (dataset['label']==1).sum()\n",
        "    sampled_from_0s = dataset[dataset['label']==0].sample(n = 2*min_len, random_state=42)\n",
        "    return pd.concat([dataset[dataset['label']==1], sampled_from_0s])\n",
        "\n",
        "def read_dataset(path):\n",
        "    dataset = pd.read_csv(path)\n",
        "    dataset = dataset[~pd.isna(dataset['comment_text'])]\n",
        "    dataset['label'] = dataset.values[:,2:].sum(axis=1) > 0\n",
        "    dataset['label'] = dataset['label'].astype(int)\n",
        "    balanced_dataset = balance_dataset(dataset)\n",
        "    return balanced_dataset['comment_text'].tolist(), balanced_dataset['label']\n",
        "\n",
        "def data_process(data, labels):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    for sentence in data:\n",
        "        bert_inp = bert_tokenizer.__call__(sentence, max_length=36,\n",
        "                                           padding='max_length', pad_to_max_length=True,\n",
        "                                           truncation=True, return_token_type_ids=False)\n",
        "\n",
        "        input_ids.append(bert_inp['input_ids'])\n",
        "        attention_masks.append(bert_inp['attention_mask'])\n",
        "    input_ids = np.asarray(input_ids)\n",
        "    attention_masks = np.array(attention_masks)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return input_ids, attention_masks, labels\n",
        "\n",
        "def pre_process_dataset(values):\n",
        "    new_values = list()\n",
        "    # Emoticons\n",
        "    emoticons = [':-)', ':)', '(:', '(-:', ':))', '((:', ':-D', ':D', 'X-D', 'XD', 'xD', 'xD', '<3', '</3', ':\\*',\n",
        "                 ';-)',\n",
        "                 ';)', ';-D', ';D', '(;', '(-;', ':-(', ':(', '(:', '(-:', ':,(', ':\\'(', ':\"(', ':((', ':D', '=D',\n",
        "                 '=)',\n",
        "                 '(=', '=(', ')=', '=-O', 'O-=', ':o', 'o:', 'O:', 'O:', ':-o', 'o-:', ':P', ':p', ':S', ':s', ':@',\n",
        "                 ':>',\n",
        "                 ':<', '^_^', '^.^', '>.>', 'T_T', 'T-T', '-.-', '*.*', '~.~', ':*', ':-*', 'xP', 'XP', 'XP', 'Xp',\n",
        "                 ':-|',\n",
        "                 ':->', ':-<', '$_$', '8-)', ':-P', ':-p', '=P', '=p', ':*)', '*-*', 'B-)', 'O.o', 'X-(', ')-X']\n",
        "\n",
        "    for value in values:\n",
        "        # Remove dots\n",
        "        text = value.replace(\".\", \"\").lower()\n",
        "        text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
        "        users = re.findall(\"[@]\\w+\", text)\n",
        "        for user in users:\n",
        "            text = text.replace(user, \"<user>\")\n",
        "        urls = re.findall(r'(https?://[^\\s]+)', text)\n",
        "        if len(urls) != 0:\n",
        "            for url in urls:\n",
        "                text = text.replace(url, \"<url >\")\n",
        "        for emo in text:\n",
        "            if emo in emoji.UNICODE_EMOJI:\n",
        "                text = text.replace(emo, \"<emoticon >\")\n",
        "        for emo in emoticons:\n",
        "            text = text.replace(emo, \"<emoticon >\")\n",
        "        numbers = re.findall('[0-9]+', text)\n",
        "        for number in numbers:\n",
        "            text = text.replace(number, \"<number >\")\n",
        "        text = text.replace('#', \"<hashtag >\")\n",
        "        text = re.sub(r\"([?.!,¿])\", r\" \", text)\n",
        "        text = \"\".join(l for l in text if l not in string.punctuation)\n",
        "        text = re.sub(r'[\" \"]+', \" \", text)\n",
        "        new_values.append(text)\n",
        "    return new_values\n",
        "\n",
        "def load_and_process(path):\n",
        "    data, labels = read_dataset(path)\n",
        "    num_of_labels = len(labels.unique())\n",
        "    input_ids, attention_masks, labels = data_process(pre_process_dataset(data), labels)\n",
        "\n",
        "    return input_ids, attention_masks, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMtg3PWcknED",
        "outputId": "aa65b602-d273-4e4f-f9d4-8cf3b63f0c20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 / 3:\n",
            "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.008\n",
            "\n",
            "Evaluating...\n",
            "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.006\n",
            "\n",
            "Training Loss: 0.008\n",
            "Validation Loss: 0.006\n",
            "\n",
            "Epoch 2 / 3:\n",
            "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.005\n",
            "\n",
            "Evaluating...\n",
            "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.007\n",
            "\n",
            "Training Loss: 0.005\n",
            "Validation Loss: 0.007\n",
            "\n",
            "Epoch 3 / 3:\n",
            "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.003\n",
            "\n",
            "Evaluating...\n",
            "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.008\n",
            "\n",
            "Training Loss: 0.003\n",
            "Validation Loss: 0.008\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total = len(train_dataloader)\n",
        "\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        step = i+1\n",
        "        percent = \"{0:.3f}\".format(100 * (step / float(total)))\n",
        "        lossp = \"{0:.3f}\".format(total_loss/(step*batch_size))\n",
        "        filledLength = int(100 * step // total)\n",
        "        bar = '█' * filledLength + '>'  *(filledLength < 100) + '.' * (99 - filledLength)\n",
        "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "        del batch\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        model.zero_grad()\n",
        "        preds = model(sent_id.to(device).long(), mask)\n",
        "        loss = cross_entropy(preds, labels)\n",
        "        total_loss += float(loss.item())\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = total_loss / (len(train_dataloader)*batch_size)\n",
        "    \n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "    print(\"\\n\\nEvaluating...\")\n",
        "    model.eval()\n",
        "    total_loss= 0\n",
        "    total = len(val_dataloader)\n",
        "    for i, batch in enumerate(val_dataloader):\n",
        "        step = i+1\n",
        "        percent = \"{0:.3f}\".format(100 * (step / float(total)))\n",
        "        lossp = \"{0:.3f}\".format(total_loss/(step*batch_size))\n",
        "        filledLength = int(100 * step // total)\n",
        "        bar = '█' * filledLength + '>' * (filledLength < 100) + '.' * (99 - filledLength)\n",
        "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
        "        # push the batch to gpu\n",
        "        batch = [t.to(device) for t in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "        del batch\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        with torch.no_grad():\n",
        "            preds = model(sent_id, mask)\n",
        "            loss = cross_entropy(preds, labels)\n",
        "            total_loss += float(loss.item())\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    val_avg_loss = total_loss / (len(val_dataloader)*batch_size)\n",
        "\n",
        "    return val_avg_loss\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load the dataset\n",
        "input_ids, attention_masks, labels = load_and_process(DATASET_PATH)\n",
        "df = pd.DataFrame(list(zip(input_ids, attention_masks)), columns=['input_ids', 'attention_masks'])\n",
        "\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df, labels,\n",
        "                             random_state=42, test_size=0.2, stratify=labels)\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
        "                         random_state=42, test_size=0.5, stratify=temp_labels)\n",
        "\n",
        "del temp_text\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "train_count = len(train_labels)\n",
        "test_count = len(test_labels)\n",
        "val_count = len(val_labels)\n",
        "\n",
        "\n",
        "# for train set\n",
        "train_seq = torch.tensor(train_text['input_ids'].tolist())\n",
        "train_mask = torch.tensor(train_text['attention_masks'].tolist())\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(val_text['input_ids'].tolist())\n",
        "val_mask = torch.tensor(val_text['attention_masks'].tolist())\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(test_text['input_ids'].tolist())\n",
        "test_mask = torch.tensor(test_text['attention_masks'].tolist())\n",
        "test_y = torch.tensor(test_labels.tolist())\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "model = BERT_CNN()\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Adam optimizer\n",
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "cross_entropy = nn.NLLLoss()\n",
        "\n",
        "epochs = 3\n",
        "current = 1\n",
        "# for each epoch\n",
        "while current <= epochs:\n",
        "\n",
        "    print(f'\\nEpoch {current} / {epochs}:')\n",
        "\n",
        "    # train model\n",
        "    train_loss = train()\n",
        "\n",
        "    # evaluate model\n",
        "    valid_loss = evaluate()\n",
        "    \n",
        "    print(f'\\n\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')\n",
        "\n",
        "    current = current + 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB3hhFc2lCp0"
      },
      "source": [
        "## Evaluation\n",
        "This model needs 6 GiB memory to evaluate the whole of test set, but GPU has not this size of memory as free memory because of bad fragmentation. So, I just evaluate the model on 3000 sequences of the test set.\n",
        "because of inefficiency of this model in industery(high memory consumption), I didn't partition the input set for evaluate the model on the whole of test set.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxCAawcdk-ok",
        "outputId": "74c58371-9f56-40bf-8018-a3f1c34933bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance:\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.95      0.94      1989\n",
            "           1       0.90      0.86      0.88      1011\n",
            "\n",
            "    accuracy                           0.92      3000\n",
            "   macro avg       0.91      0.90      0.91      3000\n",
            "weighted avg       0.92      0.92      0.92      3000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = model(test_seq[:3000].to(device), test_mask[:3000].to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "print(\"Performance:\")\n",
        "preds = np.argmax(preds, axis=1)\n",
        "print('Classification Report')\n",
        "print(classification_report(test_y[:3000], preds))   "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BERT-CNN.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
