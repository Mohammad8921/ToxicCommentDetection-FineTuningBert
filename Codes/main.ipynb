{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NkxEV3kzj0qM",
    "outputId": "315aa56c-5632-4a98-c324-37f2c5c06a4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "evJHanGdjziV"
   },
   "source": [
    "## BERT+CNN Model\n",
    "This model has been proposed in <https://link.springer.com/chapter/10.1007/978-3-030-36687-2_77 /> and impelemeted by [@ZeroxTM](https://github.com/ZeroxTM) in [this repository](https://github.com/ZeroxTM/BERT-CNN-Fine-Tuning-For-Hate-Speech-Detection-in-Online-Social-Media) \n",
    "\n",
    "By Alaa Grable\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMtg3PWcknED",
    "outputId": "aa65b602-d273-4e4f-f9d4-8cf3b63f0c20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.008\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.006\n",
      "\n",
      "Training Loss: 0.008\n",
      "Validation Loss: 0.006\n",
      "\n",
      "Epoch 2 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.005\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.007\n",
      "\n",
      "Training Loss: 0.005\n",
      "Validation Loss: 0.007\n",
      "\n",
      "Epoch 3 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.003\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.008\n",
      "\n",
      "Training Loss: 0.003\n",
      "Validation Loss: 0.008\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import gc\n",
    "from preprocessing import load_and_process\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from models import BERT_CNN\n",
    "\n",
    "DATASET_PATH = '/content/drive/MyDrive/NLP-project/dataset/train.csv'\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total = len(train_dataloader)\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        step = i+1\n",
    "        percent = \"{0:.3f}\".format(100 * (step / float(total)))\n",
    "        lossp = \"{0:.3f}\".format(total_loss/step)\n",
    "        filledLength = int(100 * step // total)\n",
    "        bar = '█' * filledLength + '>'  *(filledLength < 100) + '.' * (99 - filledLength)\n",
    "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        del batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        model.zero_grad()\n",
    "        preds = model(sent_id.to(device).long(), mask)\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        total_loss += float(loss.item())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\n\\nEvaluating...\")\n",
    "    model.eval()\n",
    "    total_loss= 0\n",
    "    total = len(val_dataloader)\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        step = i+1\n",
    "        percent = \"{0:.3f}\".format(100 * (step / float(total)))\n",
    "        lossp = \"{0:.3f}\".format(total_loss/step)\n",
    "        filledLength = int(100 * step // total)\n",
    "        bar = '█' * filledLength + '>' * (filledLength < 100) + '.' * (99 - filledLength)\n",
    "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        del batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask)\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_loss += float(loss.item())\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    val_avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "    return val_avg_loss\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the dataset\n",
    "input_ids, attention_masks, labels = load_and_process(DATASET_PATH)\n",
    "df = pd.DataFrame(list(zip(input_ids, attention_masks)), columns=['input_ids', 'attention_masks'])\n",
    "\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df, labels,\n",
    "                             random_state=42, test_size=0.2, stratify=labels)\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
    "                         random_state=42, test_size=0.5, stratify=temp_labels)\n",
    "\n",
    "del temp_text\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_count = len(train_labels)\n",
    "test_count = len(test_labels)\n",
    "val_count = len(val_labels)\n",
    "\n",
    "\n",
    "# for train set\n",
    "train_seq = torch.tensor(train_text['input_ids'].tolist())\n",
    "train_mask = torch.tensor(train_text['attention_masks'].tolist())\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(val_text['input_ids'].tolist())\n",
    "val_mask = torch.tensor(val_text['attention_masks'].tolist())\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(test_text['input_ids'].tolist())\n",
    "test_mask = torch.tensor(test_text['attention_masks'].tolist())\n",
    "test_y = torch.tensor(test_labels.tolist())\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "model = BERT_CNN()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "cross_entropy = nn.NLLLoss()\n",
    "\n",
    "epochs = 3\n",
    "current = 1\n",
    "# for each epoch\n",
    "while current <= epochs:\n",
    "\n",
    "    print(f'\\nEpoch {current} / {epochs}:')\n",
    "\n",
    "    # train model\n",
    "    train_loss = train()\n",
    "\n",
    "    # evaluate model\n",
    "    valid_loss = evaluate()\n",
    "    \n",
    "    print(f'\\n\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n",
    "\n",
    "    current = current + 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "iB3hhFc2lCp0"
   },
   "source": [
    "### Evaluation\n",
    "This model needs 6 GiB memory to evaluate the whole of test set, but GPU has not this size of memory as free memory because of bad fragmentation. So, I just evaluate the model on 3000 sequences of the test set.\n",
    "because of inefficiency of this model in industery(high memory consumption), I didn't partition the input set for evaluate the model on the whole of test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxCAawcdk-ok",
    "outputId": "74c58371-9f56-40bf-8018-a3f1c34933bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance:\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94      1989\n",
      "           1       0.90      0.86      0.88      1011\n",
      "\n",
      "    accuracy                           0.92      3000\n",
      "   macro avg       0.91      0.90      0.91      3000\n",
      "weighted avg       0.92      0.92      0.92      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq[:3000].to(device), test_mask[:3000].to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "print(\"Performance:\")\n",
    "preds = np.argmax(preds, axis=1)\n",
    "print('Classification Report')\n",
    "print(classification_report(test_y[:3000], preds))   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bB-vvm1NJkEm"
   },
   "source": [
    "## BERT+CNN-1D Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550,
     "referenced_widgets": [
      "49506f8a7e454976802a6a349d5db09c",
      "5f0bef4627e04532b72794cdfde9cfdd",
      "ffcca9b2a432445698bfc4b2cddd13a0",
      "9ed6170a7af94ec5b30088fef66c3175",
      "2fd900b456994bc08bf5849ca5a16913",
      "ab5a961e3cc54959bc65803a0766346b",
      "40c9469ff1f34eacb391a9c987ee51af",
      "5f51161974d149dab7268dfc6c08ba2b",
      "418fcac6fa3d4a4fa301898823d5b7dd",
      "dd2157b27cfc4ab6858a7ae8d3d9e8fc",
      "6d85bdd422e64f4d8a104ffe69dfc02a",
      "0edf6d508b32487e9f81d1849ae53aa8",
      "f717c5741cde42aaada3a6a6d0ec83b3",
      "5938b049f71543688f47ff5d547b4b80",
      "459f648f9c80480ca664dc32fe696d66",
      "2bfc2992ff184eb5846b91c3da651c92",
      "33a0c740c7fa48f494557d0cd025cc09",
      "e22dcf74132140cf859e9f47d71ca19f",
      "58596caf148b4191ada0d91824f265d8",
      "b90b1b0071134b5fbb700dd4f62f303e",
      "a1fe03107f8840f48731a892801cc353",
      "6703009c239844cc94205bd5d2433bbb"
     ]
    },
    "id": "DegSd7e6-mJt",
    "outputId": "68397e13-7e2f-4983-8235-e7992abb158e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49506f8a7e454976802a6a349d5db09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edf6d508b32487e9f81d1849ae53aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.007\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.006\n",
      "\n",
      "Training Loss: 0.007\n",
      "Validation Loss: 0.006\n",
      "\n",
      "Epoch 2 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.005\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.007\n",
      "\n",
      "Training Loss: 0.005\n",
      "Validation Loss: 0.007\n",
      "\n",
      "Epoch 3 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.003\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.008\n",
      "\n",
      "Training Loss: 0.003\n",
      "Validation Loss: 0.008\n"
     ]
    }
   ],
   "source": [
    "from models import BERT_CNN_1D\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total = len(train_dataloader)\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        step = i+1\n",
    "        percent = \"{0:.3f}\".format(100 * (step / float(total)))\n",
    "        lossp = \"{0:.3f}\".format(total_loss/step)\n",
    "        filledLength = int(100 * step // total)\n",
    "        bar = '█' * filledLength + '>'  *(filledLength < 100) + '.' * (99 - filledLength)\n",
    "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        del batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        model.zero_grad()\n",
    "        preds = model(sent_id.to(device).long(), mask)\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        total_loss += float(loss.item())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / (len(train_dataloader)*batch_size)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\n\\nEvaluating...\")\n",
    "    model.eval()\n",
    "    total_loss= 0\n",
    "    total = len(val_dataloader)\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        step = i+1\n",
    "        percent = \"{0:.3f}\".format(100 * (step / float(total)))\n",
    "        lossp = \"{0:.3f}\".format(total_loss/step)\n",
    "        filledLength = int(100 * step // total)\n",
    "        bar = '█' * filledLength + '>' * (filledLength < 100) + '.' * (99 - filledLength)\n",
    "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        del batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask)\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_loss += float(loss.item())\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    val_avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "    return val_avg_loss\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the dataset\n",
    "input_ids, attention_masks, labels = load_and_process(DATASET_PATH)\n",
    "df = pd.DataFrame(list(zip(input_ids, attention_masks)), columns=['input_ids', 'attention_masks'])\n",
    "\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df, labels,\n",
    "                             random_state=42, test_size=0.2, stratify=labels)\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
    "                         random_state=42, test_size=0.5, stratify=temp_labels)\n",
    "\n",
    "del temp_text\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_count = len(train_labels)\n",
    "test_count = len(test_labels)\n",
    "val_count = len(val_labels)\n",
    "\n",
    "\n",
    "# for train set\n",
    "train_seq = torch.tensor(train_text['input_ids'].tolist())\n",
    "train_mask = torch.tensor(train_text['attention_masks'].tolist())\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(val_text['input_ids'].tolist())\n",
    "val_mask = torch.tensor(val_text['attention_masks'].tolist())\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(test_text['input_ids'].tolist())\n",
    "test_mask = torch.tensor(test_text['attention_masks'].tolist())\n",
    "test_y = torch.tensor(test_labels.tolist())\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "model = BERT_CNN_1D()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "cross_entropy = nn.NLLLoss()\n",
    "\n",
    "epochs = 3\n",
    "current = 1\n",
    "# for each epoch\n",
    "while current <= epochs:\n",
    "\n",
    "    print(f'\\nEpoch {current} / {epochs}:')\n",
    "\n",
    "    # train model\n",
    "    train_loss = train()\n",
    "\n",
    "    # evaluate model\n",
    "    valid_loss = evaluate()\n",
    "    \n",
    "    print(f'\\n\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n",
    "\n",
    "    current = current + 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t49t6oAGJ8SG",
    "outputId": "c29d141f-4292-4d6e-9a87-ee0dfaed363e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93      3245\n",
      "           1       0.84      0.91      0.87      1623\n",
      "\n",
      "    accuracy                           0.91      4868\n",
      "   macro avg       0.90      0.91      0.90      4868\n",
      "weighted avg       0.91      0.91      0.91      4868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "preds = np.argmax(preds, axis=1)\n",
    "print(\"Performance:\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT+LSTM Model\n",
    "This model has been proposed in <https://link.springer.com/chapter/10.1007/978-3-030-36687-2_77 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DegSd7e6-mJt",
    "outputId": "263254d0-37f5-430c-94e1-cac9ab7003e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.007\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.006\n",
      "\n",
      "Training Loss: 0.007\n",
      "Validation Loss: 0.006\n",
      "\n",
      "Epoch 2 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.005\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.008\n",
      "\n",
      "Training Loss: 0.005\n",
      "Validation Loss: 0.008\n",
      "\n",
      "Epoch 3 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.003\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.009\n",
      "\n",
      "Training Loss: 0.003\n",
      "Validation Loss: 0.009\n"
     ]
    }
   ],
   "source": [
    "from models import BERT_LSTM\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total = len(train_dataloader)\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        step = i+1\n",
    "        percent = \"{0:.3f}\".format(100 * (step / float(total)))\n",
    "        lossp = \"{0:.3f}\".format(total_loss/step)\n",
    "        filledLength = int(100 * step // total)\n",
    "        bar = '█' * filledLength + '>'  *(filledLength < 100) + '.' * (99 - filledLength)\n",
    "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        del batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        model.zero_grad()\n",
    "        preds = model(sent_id.to(device).long(), mask)\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        total_loss += float(loss.item())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        #total_preds.append(preds.detach().cpu().numpy())\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\n\\nEvaluating...\")\n",
    "    model.eval()\n",
    "    total_loss= 0\n",
    "    total = len(val_dataloader)\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        step = i+1\n",
    "        percent = \"{0:.3f}\".format(100 * (step / float(total)))\n",
    "        lossp = \"{0:.3f}\".format(total_loss/step)\n",
    "        filledLength = int(100 * step // total)\n",
    "        bar = '█' * filledLength + '>' * (filledLength < 100) + '.' * (99 - filledLength)\n",
    "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        del batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask)\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_loss += float(loss.item())\n",
    "            #total_preds.append(preds.detach().cpu().numpy())\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    val_avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "    return val_avg_loss\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the dataset\n",
    "input_ids, attention_masks, labels = load_and_process(DATASET_PATH)\n",
    "df = pd.DataFrame(list(zip(input_ids, attention_masks)), columns=['input_ids', 'attention_masks'])\n",
    "\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df, labels,\n",
    "                             random_state=42, test_size=0.2, stratify=labels)\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
    "                         random_state=42, test_size=0.5, stratify=temp_labels)\n",
    "\n",
    "del temp_text\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_count = len(train_labels)\n",
    "test_count = len(test_labels)\n",
    "val_count = len(val_labels)\n",
    "\n",
    "\n",
    "# for train set\n",
    "train_seq = torch.tensor(train_text['input_ids'].tolist())\n",
    "train_mask = torch.tensor(train_text['attention_masks'].tolist())\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(val_text['input_ids'].tolist())\n",
    "val_mask = torch.tensor(val_text['attention_masks'].tolist())\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(test_text['input_ids'].tolist())\n",
    "test_mask = torch.tensor(test_text['attention_masks'].tolist())\n",
    "test_y = torch.tensor(test_labels.tolist())\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "model = BERT_LSTM()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "cross_entropy = nn.NLLLoss()\n",
    "\n",
    "epochs = 3\n",
    "current = 1\n",
    "# for each epoch\n",
    "while current <= epochs:\n",
    "\n",
    "    print(f'\\nEpoch {current} / {epochs}:')\n",
    "\n",
    "    # train model\n",
    "    train_loss = train()\n",
    "\n",
    "    # evaluate model\n",
    "    valid_loss = evaluate()\n",
    "    \n",
    "    print(f'\\n\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n",
    "\n",
    "    current = current + 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ZtcZbXIcVn9",
    "outputId": "57266d2c-885d-4a6b-8839-368338449006"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance:\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94      3245\n",
      "           1       0.87      0.88      0.87      1623\n",
      "\n",
      "    accuracy                           0.91      4868\n",
      "   macro avg       0.90      0.91      0.90      4868\n",
      "weighted avg       0.92      0.91      0.92      4868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "print(\"Performance:\")\n",
    "# model's performance\n",
    "preds = np.argmax(preds, axis=1)\n",
    "print('Classification Report')\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT+LSTM-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619,
     "referenced_widgets": [
      "d335bb9436174e7da9b829cf65f0e105",
      "70f93ea7158146ccab98406e314021ba",
      "22ae1f915d3d4d4da5bb29b4fffd3263",
      "abb24b0c9ab944e1a8b26b04b5de93b6",
      "77a34b2fd72d4833b5cac3b7830e3b7a",
      "d8436c2766cd46b98b1113cf3a275fe7",
      "c01f0416bd5f4878851991741a0ddb7d",
      "87ae6e2f07d148b4b953207f3382c8ae",
      "f22af7406e504b71b15a730129fc826f",
      "552ac8d194e84bdaba541444a44de3ca",
      "f05e1854e98448d0a0f38fa19ab2c179",
      "f09e9cfc5e1049bdbc093d7c844de9a3",
      "3532988890a3424f8e160d30f8326d73",
      "5c811205a88743019441f9ed3a2426b4",
      "b8e87b23938e4e9891fc97d124c4de38",
      "539542f47cc0405289dcb845447a6e5f",
      "b39e69ae943747978e2e4a90db9ed446",
      "efd1ebc41cf04013a7b07ee29b65d359",
      "f56d4160307947bcad358bf1039799b6",
      "7080190dfa8c4773a7cf8b5b042ccf1b",
      "0fc5c7bc9dc847f8a3ab0ade2c44bc61",
      "dad4200cddcc46eead06841787a07bdf",
      "fe474c7eff074b12a63ad359323cb800",
      "cd5a72ea74584437928ba90281c901f1",
      "b2b34444e1bc4476b84879a82ad98b1c",
      "c6f070452428468db10a9ca560f1d173",
      "9ceaefbb4bb047b4948fb89f3a9d9e22",
      "0e328987361749f194ca7742934e4a51",
      "07f98890a66f40589703e8a1d4cce46c",
      "15a6881e82ed4969bdb9273659202a2e",
      "6af7eb0881f44a2d8f0fb545a6ca9efe",
      "002b23111f6844fc97e406cb63685780",
      "8a2751f50c754d8a844db3ea8456aa88"
     ]
    },
    "id": "DegSd7e6-mJt",
    "outputId": "a790bc66-8809-44b7-e598-1e8630246b0c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d335bb9436174e7da9b829cf65f0e105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09e9cfc5e1049bdbc093d7c844de9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe474c7eff074b12a63ad359323cb800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.008\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.007\n",
      "\n",
      "Training Loss: 0.008\n",
      "Validation Loss: 0.007\n",
      "\n",
      "Epoch 2 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.005\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.008\n",
      "\n",
      "Training Loss: 0.005\n",
      "Validation Loss: 0.008\n",
      "\n",
      "Epoch 3 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.003\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.009\n",
      "\n",
      "Training Loss: 0.003\n",
      "Validation Loss: 0.009\n"
     ]
    }
   ],
   "source": [
    "from models import BERT_LSTM_CNN\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total = len(train_dataloader)\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        step = i+1\n",
    "        percent = \"{0:.3f}\".format(100 * (step / float(total)))\n",
    "        lossp = \"{0:.3f}\".format(total_loss/step)\n",
    "        filledLength = int(100 * step // total)\n",
    "        bar = '█' * filledLength + '>'  *(filledLength < 100) + '.' * (99 - filledLength)\n",
    "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        del batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        model.zero_grad()\n",
    "        preds = model(sent_id.to(device).long(), mask)\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        total_loss += float(loss.item())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\n\\nEvaluating...\")\n",
    "    model.eval()\n",
    "    total_loss= 0\n",
    "    total = len(val_dataloader)\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        step = i+1\n",
    "        percent = \"{0:.3f}\".format(100 * (step / float(total)))\n",
    "        lossp = \"{0:.3f}\".format(total_loss/step)\n",
    "        filledLength = int(100 * step // total)\n",
    "        bar = '█' * filledLength + '>' * (filledLength < 100) + '.' * (99 - filledLength)\n",
    "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        del batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask)\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_loss += float(loss.item())\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    val_avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "    return val_avg_loss\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the dataset\n",
    "input_ids, attention_masks, labels = load_and_process(DATASET_PATH)\n",
    "df = pd.DataFrame(list(zip(input_ids, attention_masks)), columns=['input_ids', 'attention_masks'])\n",
    "\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df, labels,\n",
    "                             random_state=42, test_size=0.2, stratify=labels)\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
    "                         random_state=42, test_size=0.5, stratify=temp_labels)\n",
    "\n",
    "del temp_text\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_count = len(train_labels)\n",
    "test_count = len(test_labels)\n",
    "val_count = len(val_labels)\n",
    "\n",
    "\n",
    "# for train set\n",
    "train_seq = torch.tensor(train_text['input_ids'].tolist())\n",
    "train_mask = torch.tensor(train_text['attention_masks'].tolist())\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(val_text['input_ids'].tolist())\n",
    "val_mask = torch.tensor(val_text['attention_masks'].tolist())\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(test_text['input_ids'].tolist())\n",
    "test_mask = torch.tensor(test_text['attention_masks'].tolist())\n",
    "test_y = torch.tensor(test_labels.tolist())\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "model = BERT_LSTM_CNN()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "cross_entropy = nn.NLLLoss()\n",
    "\n",
    "epochs = 3\n",
    "current = 1\n",
    "# for each epoch\n",
    "while current <= epochs:\n",
    "\n",
    "    print(f'\\nEpoch {current} / {epochs}:')\n",
    "\n",
    "    # train model\n",
    "    train_loss = train()\n",
    "\n",
    "    # evaluate model\n",
    "    valid_loss = evaluate()\n",
    "    \n",
    "    print(f'\\n\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n",
    "\n",
    "    current = current + 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6oMCpUDB41bc",
    "outputId": "1cb063f0-cbc9-4347-a077-dc0697c81294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance:\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94      3245\n",
      "           1       0.91      0.85      0.88      1623\n",
      "\n",
      "    accuracy                           0.92      4868\n",
      "   macro avg       0.92      0.90      0.91      4868\n",
      "weighted avg       0.92      0.92      0.92      4868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "print(\"Performance:\")\n",
    "# model's performance\n",
    "preds = np.argmax(preds, axis=1)\n",
    "print('Classification Report')\n",
    "print(classification_report(test_y, preds))   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KkTl5_WU-CXB"
   },
   "source": [
    "## BERT+CNN-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DegSd7e6-mJt",
    "outputId": "b73da70d-db04-469c-c13b-33fc1846cf1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.008\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.006\n",
      "\n",
      "Training Loss: 0.008\n",
      "Validation Loss: 0.006\n",
      "\n",
      "Epoch 2 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.005\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.007\n",
      "\n",
      "Training Loss: 0.005\n",
      "Validation Loss: 0.007\n",
      "\n",
      "Epoch 3 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.003\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.008\n",
      "\n",
      "Training Loss: 0.003\n",
      "Validation Loss: 0.008\n"
     ]
    }
   ],
   "source": [
    "from models import BERT_CNN_LSTM\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total = len(train_dataloader)\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        step = i+1\n",
    "        percent = \"{0:.3f}\".format(100 * (step / float(total)))\n",
    "        lossp = \"{0:.3f}\".format(total_loss/step)\n",
    "        filledLength = int(100 * step // total)\n",
    "        bar = '█' * filledLength + '>'  *(filledLength < 100) + '.' * (99 - filledLength)\n",
    "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        del batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        model.zero_grad()\n",
    "        preds = model(sent_id.to(device).long(), mask)\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        total_loss += float(loss.item())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\n\\nEvaluating...\")\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total = len(val_dataloader)\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        step = i+1\n",
    "        percent = \"{0:.3f}\".format(100 * (step / float(total)))\n",
    "        lossp = \"{0:.3f}\".format(total_loss/step)\n",
    "        filledLength = int(100 * step // total)\n",
    "        bar = '█' * filledLength + '>' * (filledLength < 100) + '.' * (99 - filledLength)\n",
    "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        del batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask)\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_loss += float(loss.item())\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    val_avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "    return val_avg_loss\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the dataset\n",
    "input_ids, attention_masks, labels = load_and_process(DATASET_PATH)\n",
    "df = pd.DataFrame(list(zip(input_ids, attention_masks)), columns=['input_ids', 'attention_masks'])\n",
    "\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df, labels,\n",
    "                             random_state=42, test_size=0.2, stratify=labels)\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
    "                         random_state=42, test_size=0.5, stratify=temp_labels)\n",
    "\n",
    "del temp_text\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_count = len(train_labels)\n",
    "test_count = len(test_labels)\n",
    "val_count = len(val_labels)\n",
    "\n",
    "\n",
    "# for train set\n",
    "train_seq = torch.tensor(train_text['input_ids'].tolist())\n",
    "train_mask = torch.tensor(train_text['attention_masks'].tolist())\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(val_text['input_ids'].tolist())\n",
    "val_mask = torch.tensor(val_text['attention_masks'].tolist())\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(test_text['input_ids'].tolist())\n",
    "test_mask = torch.tensor(test_text['attention_masks'].tolist())\n",
    "test_y = torch.tensor(test_labels.tolist())\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "model = BERT_CNN_LSTM()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "cross_entropy = nn.NLLLoss()\n",
    "\n",
    "epochs = 3\n",
    "current = 1\n",
    "# for each epoch\n",
    "while current <= epochs:\n",
    "\n",
    "    print(f'\\nEpoch {current} / {epochs}:')\n",
    "\n",
    "    # train model\n",
    "    train_loss = train()\n",
    "\n",
    "    # evaluate model\n",
    "    valid_loss = evaluate()\n",
    "    \n",
    "    print(f'\\n\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n",
    "\n",
    "    current = current + 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ZtcZbXIcVn9",
    "outputId": "96580a76-e461-43f9-cd3e-d08ed0d0213f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance:\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94      3245\n",
      "           1       0.87      0.88      0.88      1623\n",
      "\n",
      "    accuracy                           0.92      4868\n",
      "   macro avg       0.91      0.91      0.91      4868\n",
      "weighted avg       0.92      0.92      0.92      4868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "print(\"Performance:\")\n",
    "# model's performance\n",
    "preds = np.argmax(preds, axis=1)\n",
    "print('Classification Report')\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT+2CNN-1D Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DegSd7e6-mJt",
    "outputId": "de614f3d-71f2-4e17-d926-2781c16b1184"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.007\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.007\n",
      "\n",
      "Training Loss: 0.007\n",
      "Validation Loss: 0.007\n",
      "\n",
      "Epoch 2 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.005\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.006\n",
      "\n",
      "Training Loss: 0.005\n",
      "Validation Loss: 0.006\n",
      "\n",
      "Epoch 3 / 3:\n",
      "Batch 1217/1217 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.003\n",
      "\n",
      "Evaluating...\n",
      "Batch 153/153 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.000% complete, loss=0.007\n",
      "\n",
      "Training Loss: 0.003\n",
      "Validation Loss: 0.007\n"
     ]
    }
   ],
   "source": [
    "from models import BERT_2CNN_1D\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total = len(train_dataloader)\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        step = i+1\n",
    "        percent = \"{0:.3f}\".format(100 * (step / float(total)))\n",
    "        lossp = \"{0:.3f}\".format(total_loss/step)\n",
    "        filledLength = int(100 * step // total)\n",
    "        bar = '█' * filledLength + '>'  *(filledLength < 100) + '.' * (99 - filledLength)\n",
    "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        del batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        model.zero_grad()\n",
    "        preds = model(sent_id.to(device).long(), mask)\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        total_loss += float(loss.item())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\n\\nEvaluating...\")\n",
    "    model.eval()\n",
    "    total_loss= 0\n",
    "    total = len(val_dataloader)\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        step = i+1\n",
    "        percent = \"{0:.3f}\".format(100 * (step / float(total)))\n",
    "        lossp = \"{0:.3f}\".format(total_loss/step)\n",
    "        filledLength = int(100 * step // total)\n",
    "        bar = '█' * filledLength + '>' * (filledLength < 100) + '.' * (99 - filledLength)\n",
    "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        del batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask)\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_loss += float(loss.item())\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    val_avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "    return val_avg_loss\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the dataset\n",
    "input_ids, attention_masks, labels = load_and_process(DATASET_PATH)\n",
    "df = pd.DataFrame(list(zip(input_ids, attention_masks)), columns=['input_ids', 'attention_masks'])\n",
    "\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df, labels,\n",
    "                             random_state=42, test_size=0.2, stratify=labels)\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
    "                         random_state=42, test_size=0.5, stratify=temp_labels)\n",
    "\n",
    "del temp_text\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_count = len(train_labels)\n",
    "test_count = len(test_labels)\n",
    "val_count = len(val_labels)\n",
    "\n",
    "\n",
    "# for train set\n",
    "train_seq = torch.tensor(train_text['input_ids'].tolist())\n",
    "train_mask = torch.tensor(train_text['attention_masks'].tolist())\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(val_text['input_ids'].tolist())\n",
    "val_mask = torch.tensor(val_text['attention_masks'].tolist())\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(test_text['input_ids'].tolist())\n",
    "test_mask = torch.tensor(test_text['attention_masks'].tolist())\n",
    "test_y = torch.tensor(test_labels.tolist())\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "model = BERT_2CNN_1D()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "cross_entropy = nn.NLLLoss()\n",
    "\n",
    "epochs = 3\n",
    "current = 1\n",
    "# for each epoch\n",
    "while current <= epochs:\n",
    "\n",
    "    print(f'\\nEpoch {current} / {epochs}:')\n",
    "\n",
    "    # train model\n",
    "    train_loss = train()\n",
    "\n",
    "    # evaluate model\n",
    "    valid_loss = evaluate()\n",
    "    \n",
    "    print(f'\\n\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n",
    "\n",
    "    current = current + 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ZtcZbXIcVn9",
    "outputId": "ee1ae448-9918-4306-b82a-4d67c65dde6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance:\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94      3245\n",
      "           1       0.89      0.87      0.88      1623\n",
      "\n",
      "    accuracy                           0.92      4868\n",
      "   macro avg       0.91      0.91      0.91      4868\n",
      "weighted avg       0.92      0.92      0.92      4868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "print(\"Performance:\")\n",
    "# model's performance\n",
    "preds = np.argmax(preds, axis=1)\n",
    "print('Classification Report')\n",
    "print(classification_report(test_y, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.8 (tags/v3.7.8:4b47a5b6ba, Jun 28 2020, 08:53:46) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96589047efed64a9c8c3a7bd39139c2dcad374972fa2c1e97fecd81fa1d46ebd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
